# Important: Be careful when modifying this file! 
# The fields in file will be overridden by the dataset dependent config file in the `configurations/dataset_experiment` folder so consider making changes there instead! 


defaults:
  - base_pytorch_exp

tasks: [training]

ema:
  enable: True
  decay: 0.9999
    
training:
  lr: 1e-4
  precision: 16-mixed # 16-mixed
  batch_size: 8 # DiT-B/2 = 16, DiT-L/2 = 8
  # max_steps: 200000
  max_epochs: 1
  dataloader:
    num_workers: 24
    shuffle: True
  checkpointing:
    every_n_train_steps: 5000
    every_n_epochs: null
    save_top_k: -1
  optim:
    optimizer_beta: [0.9, 0.99]
    gradient_clip_val: 1.0
  weight_decay: 1e-3
  lr_scheduler:
    name: constant_with_warmup
    num_warmup_steps: 10000

  compile: ${algorithm.compile}
  splits_included: ['training']

validation:
  batch_size: 1
  precision: 32
  val_every_n_step: 10000 # if set to float, then it means [0.0, 1.0] fraction of training steps; otherwise, it means fixed number of training steps
  val_every_n_epoch: null
  limit_batch: 1.0
  dataloader:
    num_workers: 12
    shuffle: false

  # splits_included: ["validation"]
  # splits_included: ["validation", "ood-basic-ability-len128", "memory_test_case_len_256_no_noise_800cases"] # "easy_rotate_and_back_len32",
  # splits_included: ["build_house"] # "easy_rotate_and_back_len32",
  splits_included: ["find_cave"]
  # splits_included: ["validation", "ood-basic-ability-len128", "memory_test_case_len_256_no_noise_800cases", "find_cave"] # "easy_rotate_and_back_len32",


test:
  batch_size: 16
  limit_batch: 1.0
  dataloader:
    num_workers: 1
reload_dataloaders_every_n_epochs: 0 # not resumable then
find_unused_parameters: false