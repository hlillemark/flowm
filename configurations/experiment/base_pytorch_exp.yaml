# inherites from base_experiment.yaml
# most of the options have docs at https://lightning.ai/docs/pytorch/stable/common/trainer.html

defaults:
  - base_exp

tasks: [training] # tasks to run sequantially, change when your project has multiple stages and you want to run only a subset of them.
num_nodes: 1 # number of gpu servers used in large scale distributed training

calculate_throughput: false
throughput:
  target_attr: null
  core_module_attr: null
  warmup_steps: 63
  log_wandb_table: true
  module_specs: []
training:
  precision: 16-mixed # set float precision, 16-mixed is faster while 32 is more stable
  compile: False # whether to compile the model with torch.compile
  lr: 0.001 # learning rate
  lr_scheduler:
    name: constant_with_warmup
    num_warmup_steps: 5000
  lr_map: null # we assign different lr to different module
  weight_decay: 0 # default weight decay for adam is 0.
  batch_size: 16 # training batch size; effective batch size is this number * gpu * nodes iff using distributed training
  max_epochs: 10 # set to -1 to train forever
  max_steps: -1 # set to -1 to train forever, will override max_epochs
  max_time: null # set to something like "00:12:00:00" to enable
  dataloader:
    num_workers: 16 # number of CPU threads for data preprocessing.
    shuffle: True # whether training data will be shuffled
    pin_memory: True
    prefetch_factor: 2
  optim:
    accumulate_grad_batches: 1 # accumulate gradients for n batches before backprop
    gradient_clip_val: 0 # clip gradients with norm above this value, set to 0 to disable
    optimizer_beta: [0.9, 0.999] # default optimizer beta for adam
    opt_name: adamw # 
  checkpointing:
    # these are arguments to pytorch lightning's callback, `ModelCheckpoint` class
    every_n_train_steps: 5000 # save a checkpoint every n train steps
    every_n_epochs: null # mutually exclusive with ``every_n_train_steps`` and ``train_time_interval``
    train_time_interval: null # in format of "00:12:00:00", mutually exclusive with ``every_n_train_steps`` and ``every_n_epochs``.
    enable_version_counter: False # NOTE: If this is ``False``, later checkpoint will be overwrite previous ones.
    save_on_train_epoch_end: False
    # NOTE: if using save_top_k != -1 (all), then set monitor and mode properly. it may not be named val_loss for your expt.
    # monitor: val_loss
    # mode: min
    save_top_k: -1
    # filename: '{epoch:02d}-{step:06d}-{val_loss:.2f}'

  devices: ${experiment.devices}
  strategy: ddp # deepspeed, deepspeed_stage_2, fsdp


validation:
  precision: 32
  compile: False # whether to compile the model with torch.compile
  batch_size: 16 # validation batch size per GPU; effective batch size is this number * gpu * nodes iff using distributed training
  val_every_n_step: 2000 # controls how frequent do we run validation, can be float (fraction of epoches) or int (steps) or null (if val_every_n_epoch is set)
  val_every_n_epoch: null # if you want to do validation every n epoches, requires val_every_n_step to be null.
  num_sanity_val_steps: null # number of batches to run validation before training, if null, set to int(debug)
  limit_batch: null # if null, run through validation set. Otherwise limit the number of batches to use for validation.
  dataloader:
    num_workers: 16 # number of CPU threads for data preprocessing, for validation.
    shuffle: False # whether validation data will be shuffled
    pin_memory: True
    prefetch_factor: 2
  inference_mode: True
  devices: ${experiment.devices}
  splits: ['validation'] # we may want to use multiple validation dataset (in addition to `validation` split) for validating
  strategy: ddp
  sample_during_training: false
  is_preprocessing_expt: false # easiest way to track it is to put it here.

test:
  precision: 16-mixed
  compile: False # whether to compile the model with torch.compile
  batch_size: 16 # test batch size per GPU; effective batch size is this number * gpu * nodes iff using distributed training
  limit_batch: null # if null, run through test set. Otherwise limit the number of batches to use for test.
  dataloader:
    num_workers: 16 # number of CPU threads for data preprocessing, for test.
    shuffle: False # whether test data will be shuffled
    pin_memory: True
  inference_mode: True

find_unused_parameters: False
reload_dataloaders_every_n_epochs: 0
ema:
  enable: False
  decay: 0.9999
  validate_original_weights: False # If EMA is enabled, during validation, if `validate_original_weights` is false, it will validate the EMA params; otherwise it will validate the original model weights