# @package _global_

defaults:
  - base

algorithm:
  backbone:
    num_layers: 8
    b_h_list: [1, 1, 1, 1, 2, 2, 4, 8]
    b_w_list: [1, 1, 1, 1, 2, 2, 4, 8]
    headdim: 128
    expand: 2
    d_state: 256
    share_child: True
    attn_chunk_size: 4 # in the paper they use 5 / 10, but due to some divisibility issue of mamba, we use 4 here. "we chose a frame window size of k = 10. For faster training and sampling speeds, we group frames into chunks of 5."
  